{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 12 - Writing GPU Kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The homework will be an extension of what we did in the lecture.\n",
    "This time, we will implement the matrix multiplication kernel using Triton.\n",
    "To simplify the problem, we will assume that the matrices are square and of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiplication(blockIdx, blockDim, threadIdx, A, B, C):\n",
    "    \"\"\"\n",
    "    CUDA kernel for matrix multiplication C = A @ B\n",
    "    Where A and B are square matrices of size n x n\n",
    "\n",
    "    Your task is to implement the matrix multiplication kernel\n",
    "    (see https://www.mathsisfun.com/algebra/matrix-multiplying.html,\n",
    "    chapter \"Multiplying a Matrix by Another Matrix\")\n",
    "\n",
    "    Note that right now the blocks and threads are two-dimensional, so that we can\n",
    "    process 2D data (matrices). The x-dimension indexes the rows of the\n",
    "    output matrix C, and the y-dimension indexes the columns of C.\n",
    "    This allows the kernel to execute in parallel computation of\n",
    "    ANY output element of C, with the proper indexing of a 2D matrix.\n",
    "\n",
    "    This does not have to be the most efficient implementation,\n",
    "    but it should be correct and demonstrate your understanding of\n",
    "    the problem.\n",
    "    \n",
    "    Note: Be careful with the boundary conditions for the threads.\n",
    "    We do not want to access elements outside the matrix\n",
    "    (\"step\" into the undefined memory space).\n",
    "\n",
    "    :param blockIdx: Block index in the grid\n",
    "    :param blockDim: Block dimension\n",
    "    :param threadIdx: Thread index in the block\n",
    "    :param A: First input matrix\n",
    "    :param B: Second input matrix\n",
    "    :param C: Output matrix\n",
    "    \"\"\"\n",
    "    # Calculate row and column for this thread\n",
    "    row = blockIdx.x * blockDim.x + threadIdx.x\n",
    "    col = blockIdx.y * blockDim.y + threadIdx.y\n",
    "    \n",
    "    # <your_code_here>\n",
    "    \n",
    "    # </your_code_here>\n",
    "\n",
    "\n",
    "def run_kernel(*kernel_args):\n",
    "    NUM_THREADS = 64\n",
    "\n",
    "    # Define the dimensions of the grid and block\n",
    "    DimGrid = namedtuple(\"block_dimensions\", [\"x\", \"y\"])\n",
    "    DimBlock = namedtuple(\"thread_dimensions\", [\"x\", \"y\"])\n",
    "    CurrentBlock = namedtuple(\"current_block\", [\"x\", \"y\"])\n",
    "    CurrentThread = namedtuple(\"current_thread\", [\"x\", \"y\"])\n",
    "\n",
    "    # DimGrid is an object that holds the number of blocks in the x and y dimensions\n",
    "    dim_grid = DimGrid(\n",
    "        np.ceil(n / NUM_THREADS).astype(np.int32),\n",
    "        np.ceil(n / NUM_THREADS).astype(np.int32),\n",
    "    )\n",
    "    # DimBlock is an object that holds the number of threads in the x and y dimensions\n",
    "    dim_block = DimBlock(NUM_THREADS, NUM_THREADS)\n",
    "\n",
    "    for block_i in range(dim_grid.x):\n",
    "        for block_j in range(dim_grid.y):\n",
    "            for thread_i in range(dim_block.x):\n",
    "                for thread_j in range(dim_block.y):\n",
    "                    matrix_multiplication(\n",
    "                        CurrentBlock(block_i, block_j),\n",
    "                        dim_block,\n",
    "                        CurrentThread(thread_i, thread_j),\n",
    "                        *kernel_args,\n",
    "                    )\n",
    "\n",
    "\n",
    "n = 16\n",
    "# define matrix A\n",
    "A = np.random.randn(n, n)\n",
    "# define matrix B\n",
    "B = np.random.randn(n, n)\n",
    "# the result of A*B is C\n",
    "C = np.empty((n, n))\n",
    "\n",
    "run_kernel(A, B, C)\n",
    "\n",
    "assert np.allclose(C, A @ B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "@triton.jit\n",
    "def matrix_multiplication_kernel(\n",
    "    A_ptr, B_ptr, C_ptr,\n",
    "    M, N, K,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Matrix multiplication using 2D program IDs\n",
    "    \n",
    "    Notes:\n",
    "    \n",
    "    1) This time we will use the 2D program IDs to index the matrix elements.\n",
    "    i.e.\n",
    "    ```\n",
    "    pid_row = tl.program_id(0)\n",
    "    pid_col = tl.program_id(1)\n",
    "    ```\n",
    "    \n",
    "    2) Most likely you will use an \"accumulator\" to store the result of the matrix multiplication.\n",
    "    I recommend to initialize it with zeros:\n",
    "    ```\n",
    "    acc = tl.full((BLOCK_SIZE, BLOCK_SIZE), 0.0, dtype=tl.float32)\n",
    "    ```\n",
    "    \n",
    "    This task may be potentially challenging to some of you. \n",
    "    If you have any questions, please ask in the Discord.\n",
    "    Also feel free to converse with AI to get help and seek clarification.\n",
    "    \n",
    "    \n",
    "    :param A_ptr: Pointer to matrix A (shape: M x K)\n",
    "    :param B_ptr: Pointer to matrix B (shape: K x N)\n",
    "    :param C_ptr: Pointer to output matrix C (shape: M x N)\n",
    "    :param M: Number of rows in A and C\n",
    "    :param N: Number of columns in B and C\n",
    "    :param K: Number of columns in A and rows in B\n",
    "    :param BLOCK_SIZE: Tile size for all dimensions\n",
    "    \"\"\"\n",
    "    # <your_code_here>\n",
    "    # Suggested approach:\n",
    "    # 1) Get 2D program ID\n",
    "    # 2) Compute row/column offsets\n",
    "    # 3) Create masks (x and y dimensions)\n",
    "    # 4) Initialize accumulator\n",
    "    # 5) Loop over K dimension\n",
    "    # 6) Load A and B tiles\n",
    "    # 7) Perform matrix multiplication\n",
    "    # 8) Store result\n",
    "    # </your_code_here>\n",
    "    \n",
    "# Test the implementation\n",
    "n = 16\n",
    "A = torch.randn((n, n), device='cuda')\n",
    "B = torch.randn((n, n), device='cuda')\n",
    "M, K = A.shape\n",
    "K, N = B.shape\n",
    "\n",
    "# Allocate output\n",
    "C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
    "\n",
    "# Define block size\n",
    "BLOCK_SIZE = 64\n",
    "\n",
    "# Launch kernel\n",
    "grid = lambda meta: (\n",
    "    triton.cdiv(M, meta['BLOCK_SIZE']),\n",
    "    triton.cdiv(N, meta['BLOCK_SIZE'])\n",
    ")\n",
    "\n",
    "matrix_multiplication_kernel[grid](\n",
    "    A, B, C,\n",
    "    M, N, K,\n",
    "    BLOCK_SIZE\n",
    ")\n",
    "\n",
    "assert torch.allclose(C, A @ B, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
